{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNuRDVYK6JvRB1qt/RoSApZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TarasMuzychuk/Practise_Task-3/blob/main/Task4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LEryPAa_emiO",
        "outputId": "e7a44624-804d-475d-d177-27977bad1666"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.1.tar.gz (317.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.0/317.0 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.1-py2.py3-none-any.whl size=317488491 sha256=efd5bdda5f883d645ec912bf124ec4c37d3b540b07c9149840fbd356ebd411f5\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/1d/60/2c256ed38dddce2fdd93be545214a63e02fbd8d74fb0b7f3a6\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Робота з Spark SQL\n",
        "## Складна аналітика з Spark SQL\n",
        "\n",
        "> Можна виконати або за допомогою чистого `DataFrame` API, `Pandas on Spark` API, або за допомогою чистих `SQL` запитів. Вибір за вами.\n",
        "\n",
        "Ви розробник у компанії **BikeServe**, яка займається орендою велосипедів/скутерів. У вас є певні місця (\"станції\"), де зберігаються ваші велосипеди. Якщо на станції немає вільних місць для велосипедів (хтось уже зарезервував або взяв велосипед) протягом певного періоду часу (`timeslot`), це означає, що бізнес йде чудово. Однак вам потрібно покращити обслуговування клієнтів, пропонуючи користувачам велосипеди, коли та в тому місці, де це для них найважливіше.\n",
        "\n",
        "Ваше завдання — знайти найбільш важливі (\"критичні\") пари станції та періоду часу `(stationId, timeslot)`, щоб ваш бізнес знав, куди і коли доставити більше велосипедів.\n",
        "\n",
        "Ваш результат має бути відсортований за цією *критичністю* у порядку спадання.\n",
        "\n",
        "Набір даних містить:\n",
        "* `register.csv` містить інформацію з вашої IoT системи моніторингу про кількість використаних і вільних слотів на ваших станціях оренди велосипедів. Кожен рядок відповідає одному запису про ситуацію на одній станції в певний момент часу.\n",
        "\n",
        "    Кожен рядок має такий формат:\n",
        "\n",
        "    ```bash\n",
        "    stationId\\ttimestamp\\tusedslots\\tfreeslots\n",
        "    ```\n",
        "    \n",
        "    де `timestamp` має формат `datetime`.\n",
        "\n",
        "    > Перший рядок файлу містить заголовок.\n",
        "    > Деякі дані в наборі даних пошкоджено через тимчасові збої мережі та/або вашої системи моніторингу. Це означає, що деякі рядки характеризуються \"використаними слотами (used slots) = 0\" і \"вільними слотами (free slots) = 0\". **Ці рядки необхідно відфільтрувати** перед виконанням будь-яких операцій.\n",
        "\n",
        "* `input/stations.csv` містить опис станцій.\n",
        "\n",
        "    Кожен рядок має такий формат:\n",
        "\n",
        "    ```bash\n",
        "    stationId\\tlongitude\\ttitude\\tname\n",
        "    ```\n",
        "    > Перший рядок файлу містить заголовок.\n",
        "\n",
        "### Опис завдання\n",
        "\n",
        "Кожна пара \"день тижня – година\" є \"часовим інтервалом\" (`timeslot`) і пов’язана з усіма показаннями моніторингу, пов’язаними з цією парою, незалежно від дати. Наприклад, часовий інтервал `Wednesday - 17` відповідає всім показанням, зробленим у середу з `17:00:00` до `17:59:59`.\n",
        "\n",
        "Станція $S_i$ знаходиться в критичному стані, якщо кількість вільних слотів дорівнює `0` (всі велосипеди на станції заброньовані).\n",
        "\n",
        "*Критичність* станції $S_i$ у часовому інтервалі $T_j$ визначається як:\n",
        "\n",
        "$$\n",
        "\\frac{\\text{кількість записів із числом вільних слотів, яке дорівнює нулю, для пари}_{\\left(S_i,T_j\\right)}}{\\text{загальна кількість записів для пари}_{\\left(S_i,T_j\\right)}}\n",
        "$$\n",
        "\n",
        "необхідно:\n",
        "* Обчислити значення *критичності* для кожної пари $(S_i, T_j)$.\n",
        "* Вибирати лише пари, у яких значення *критичності* перевищує \"мінімальний поріг критичності\".\n",
        "    * `Мінімальний поріг критичності` має бути параметром конфігурації програми.\n",
        "* Зберегти у вихідній папці вибрані записи, використовуючи файли `csv` (із заголовком). Зберегти лише такі атрибути:\n",
        "    * ідентифікатор станції\n",
        "    * день тижня\n",
        "    * година\n",
        "    * критичність\n",
        "    * довгота станції\n",
        "    * широта станції\n",
        "* Зберегти результати за зменшення критичності. Якщо є два або більше записів, що характеризуються однаковим значенням критичності, то додатково відсортувати по ідентифікатору станції (у порядку зростання). Якщо і станція та сама, то сортувати за днем тижня (за зростанням) і, нарешті, за годиною (за зростанням).\n",
        "\n",
        "### Поради та підказки\n",
        "\n",
        "Мова SQL, доступна в Spark SQL, має низку попередньо визначених функцій, одна з яких, `hour(timestamp)`, може використовуватися в запитах SQL або в перетворенні `selectExpr`, щоб вибрати `hour` з заданого позначка часу. Ще одна цікава функція, `date_format(timestamp,format)`, може бути використана для отримання іншої корисної інформації зі стовпця timestamp. Наприклад, у форматі `EE` можна отримати день тижня.\n",
        "\n",
        "```python\n",
        "new_df= df.selectExpr(\"date_format(timestamp,'EE') as weekday hour(timestamp) as hour\")\n",
        "```\n",
        "\n",
        "Щоб вказати, що роздільником вхідних файлів CSV є спеціальний символ `tab`, установіть параметр роздільника на `\\\\t`, викликавши `.option(\"delimiter\", \"\\\\t\")` під час читання вхідних даних."
      ],
      "metadata": {
        "id": "AgZaY7JXdMb9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Конфігурація\n",
        "\n",
        "- `number_cores`: Кількість ядер, виділених під Spark\n",
        "- `memory_gb`: Обʼєм оперативної памʼяті, виділеної під Spark (в Гб)"
      ],
      "metadata": {
        "id": "N3UDIZmKdTpV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "number_cores = 2\n",
        "memory_gb = 4"
      ],
      "metadata": {
        "id": "9zycP7sHdW4c"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "spark = (SparkSession\n",
        "    .builder\n",
        "    .appName('Spark Bikes')\n",
        "    .master(f\"local[{number_cores}]\")\n",
        "    .config(\"spark.driver.memory\", f\"{memory_gb}g\")\n",
        "    .getOrCreate())"
      ],
      "metadata": {
        "id": "p0nlqcStdYqj"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Рішення\n",
        "Прочитайте вміст вхідного файлу `register.csv` і збережіть його у DataFrame.\n",
        "\n",
        "Вхідний файл має заголовок.\n",
        "\n",
        "Схема даних:\n",
        "* station: integer (nullable = true)\n",
        "* timestamp: timestamp (nullable = true)\n",
        "* used_slots: integer (nullable = true)\n",
        "* free_slots: integer (nullable = true)"
      ],
      "metadata": {
        "id": "cfafhrIHdWG8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "WJElCcWWclbW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "774aa569-0479-4711-8509-feb2440576f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-------------------+----------+----------+\n",
            "|station|          timestamp|used_slots|free_slots|\n",
            "+-------+-------------------+----------+----------+\n",
            "|      1|2023-05-15 12:01:00|         0|        18|\n",
            "|      1|2023-05-15 12:02:00|         0|        18|\n",
            "|      1|2023-05-15 12:04:00|         0|        18|\n",
            "|      1|2023-05-15 12:06:00|         0|        18|\n",
            "|      1|2023-05-15 12:08:00|         0|        18|\n",
            "|      1|2023-05-15 12:10:00|         0|        18|\n",
            "|      1|2023-05-15 12:12:00|         0|        18|\n",
            "|      1|2023-05-15 12:14:00|         0|        18|\n",
            "|      1|2023-05-15 12:16:00|         0|        18|\n",
            "|      1|2023-05-15 12:18:00|         0|        18|\n",
            "|      1|2023-05-15 12:20:00|         2|        16|\n",
            "|      1|2023-05-15 12:22:00|         3|        15|\n",
            "|      1|2023-05-15 12:24:00|         3|        15|\n",
            "|      1|2023-05-15 12:26:00|         3|        15|\n",
            "|      1|2023-05-15 12:28:00|         4|        14|\n",
            "|      1|2023-05-15 12:30:00|         0|        12|\n",
            "|      1|2023-05-15 12:32:00|         4|        14|\n",
            "|      1|2023-05-15 12:34:00|         4|        14|\n",
            "|      1|2023-05-15 12:36:00|         4|        14|\n",
            "|      1|2023-05-15 12:38:00|         4|        14|\n",
            "+-------+-------------------+----------+----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# ваш код\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Ініціалізуємо SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Read Register CSV\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Створюємо тимчасовий вид таблиці з файлу CSV\n",
        "spark.read.format(\"csv\") \\\n",
        "    .option(\"header\", \"true\") \\\n",
        "    .option(\"inferSchema\", \"true\") \\\n",
        "    .option(\"delimiter\", \"\\t\") \\\n",
        "    .load(\"register.csv\") \\\n",
        "    .createOrReplaceTempView(\"register\")\n",
        "\n",
        "# Виконуємо SQL-запит для вибору всіх даних з тимчасової таблиці\n",
        "register_df = spark.sql(\"SELECT * FROM register\")\n",
        "\n",
        "# Виводимо результат\n",
        "register_df.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Видаліть рядки де одночасно `free_slots = 0` та `used_slots = 0`"
      ],
      "metadata": {
        "id": "ERg32xSQdqPj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ваш код\n",
        "# Фільтруємо рядки, де free_slots не дорівнює 0 та used_slots не дорівнює 0\n",
        "register_df_is_filtered = spark.sql(\"\"\"\n",
        "    SELECT *\n",
        "    FROM register\n",
        "    WHERE free_slots != 0 AND used_slots != 0\n",
        "\"\"\")\n",
        "\n",
        "# Виводимо для перевірки\n",
        "register_df_is_filtered.show()\n"
      ],
      "metadata": {
        "id": "b2YmKfEEdtWj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf0f280e-687d-4960-cb39-52cce25396a1"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-------------------+----------+----------+\n",
            "|station|          timestamp|used_slots|free_slots|\n",
            "+-------+-------------------+----------+----------+\n",
            "|      1|2023-05-15 12:20:00|         2|        16|\n",
            "|      1|2023-05-15 12:22:00|         3|        15|\n",
            "|      1|2023-05-15 12:24:00|         3|        15|\n",
            "|      1|2023-05-15 12:26:00|         3|        15|\n",
            "|      1|2023-05-15 12:28:00|         4|        14|\n",
            "|      1|2023-05-15 12:32:00|         4|        14|\n",
            "|      1|2023-05-15 12:34:00|         4|        14|\n",
            "|      1|2023-05-15 12:36:00|         4|        14|\n",
            "|      1|2023-05-15 12:38:00|         4|        14|\n",
            "|      1|2023-05-15 12:40:00|         5|        13|\n",
            "|      1|2023-05-15 12:42:00|         6|        12|\n",
            "|      1|2023-05-15 12:46:00|         6|        12|\n",
            "|      1|2023-05-15 12:48:00|         7|        11|\n",
            "|      1|2023-05-15 12:50:00|         7|        11|\n",
            "|      1|2023-05-15 12:52:00|         7|        11|\n",
            "|      1|2023-05-15 12:54:00|         8|        10|\n",
            "|      1|2023-05-15 12:56:00|         9|         9|\n",
            "|      1|2023-05-15 12:58:00|         9|         9|\n",
            "|      1|2023-05-15 13:00:00|         9|         9|\n",
            "|      1|2023-05-15 13:02:00|         9|         9|\n",
            "+-------+-------------------+----------+----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Нам потрібен логічний маркер, щоб побачити, заповнена станція чи ні. Це можна зробити за допомогою UDF під назвою `full(free_slots: int)`, яка повертає\n",
        "* 1, якщо `free_slots` дорівнює 0\n",
        "* 0, якщо `free_slots` більше 0\n",
        "\n",
        "> Якщо ви використовуєте Pandas on Spark API, то треба самостійно застосувати цю функцію (або переписати її)"
      ],
      "metadata": {
        "id": "Rw3T1OgCdvc7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ваш код\n",
        "from pyspark.sql.types import IntegerType\n",
        "\n",
        "# Визначаємо SQL функцію для перевірки стану станції\n",
        "spark.udf.register(\"full_function\", lambda free_slots: 1 if free_slots == 0 else 0, IntegerType())\n",
        "\n",
        "# Додаємо новий стовпець, який використовує SQL функцію\n",
        "register_df_is_filtered.createOrReplaceTempView(\"register_with_udf\")\n",
        "register_with_is_full = spark.sql(\"\"\"\n",
        "    SELECT *, full_function(free_slots) AS full\n",
        "    FROM register_with_udf\n",
        "\"\"\")\n",
        "\n",
        "# Виводимо результат\n",
        "register_with_is_full.show()\n"
      ],
      "metadata": {
        "id": "8cJeNxjwdyf7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e2deb82-c28c-42b2-e0a4-980ec15363fa"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-------------------+----------+----------+----+\n",
            "|station|          timestamp|used_slots|free_slots|full|\n",
            "+-------+-------------------+----------+----------+----+\n",
            "|      1|2023-05-15 12:20:00|         2|        16|   0|\n",
            "|      1|2023-05-15 12:22:00|         3|        15|   0|\n",
            "|      1|2023-05-15 12:24:00|         3|        15|   0|\n",
            "|      1|2023-05-15 12:26:00|         3|        15|   0|\n",
            "|      1|2023-05-15 12:28:00|         4|        14|   0|\n",
            "|      1|2023-05-15 12:32:00|         4|        14|   0|\n",
            "|      1|2023-05-15 12:34:00|         4|        14|   0|\n",
            "|      1|2023-05-15 12:36:00|         4|        14|   0|\n",
            "|      1|2023-05-15 12:38:00|         4|        14|   0|\n",
            "|      1|2023-05-15 12:40:00|         5|        13|   0|\n",
            "|      1|2023-05-15 12:42:00|         6|        12|   0|\n",
            "|      1|2023-05-15 12:46:00|         6|        12|   0|\n",
            "|      1|2023-05-15 12:48:00|         7|        11|   0|\n",
            "|      1|2023-05-15 12:50:00|         7|        11|   0|\n",
            "|      1|2023-05-15 12:52:00|         7|        11|   0|\n",
            "|      1|2023-05-15 12:54:00|         8|        10|   0|\n",
            "|      1|2023-05-15 12:56:00|         9|         9|   0|\n",
            "|      1|2023-05-15 12:58:00|         9|         9|   0|\n",
            "|      1|2023-05-15 13:00:00|         9|         9|   0|\n",
            "|      1|2023-05-15 13:02:00|         9|         9|   0|\n",
            "+-------+-------------------+----------+----------+----+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Створіть DataFrame з такою схемою:\n",
        "* station: integer (nullable = true)\n",
        "* dayofweek: string (nullable = true)\n",
        "* hour: integer (nullable = true)\n",
        "* fullstatus: integer (nullable = true) - 1 = full, 0 = non-full"
      ],
      "metadata": {
        "id": "p_i8yUr3d1qd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ваш код\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, to_date, hour, minute, second, concat_ws\n",
        "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
        "\n",
        "# Ініціалізуємо SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Create DataFrame\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Описуємо схему для DataFrame\n",
        "napolnenie = StructType([\n",
        "    StructField(\"station\", IntegerType(), nullable=True),\n",
        "    StructField(\"hour\", StringType(), nullable=True)\n",
        "])\n",
        "\n",
        "# Створюємо DataFrame з вказаною схемою\n",
        "register_df = spark.createDataFrame([], napolnenie )\n",
        "\n",
        "# Додаємо необхідні стовпці до DataFrame\n",
        "register_df = register_with_is_full.withColumn(\"dayofweek\", to_date(col(\"timestamp\"))) \\\n",
        "                         .withColumn(\"hour\", hour(col(\"timestamp\"))) \\\n",
        "                         .withColumn(\"minute\", minute(col(\"timestamp\"))) \\\n",
        "                         .withColumn(\"second\", second(col(\"timestamp\"))) \\\n",
        "                         .withColumn(\"hour\",\n",
        "                                     concat_ws(\":\", col(\"hour\"), col(\"minute\"), col(\"second\"))) \\\n",
        "                         .withColumn(\"fullstatus\", col(\"full\"))\n",
        "\n",
        "# Вибираємо тільки необхідні колонки: station, dayofweek, hour, fullstatus\n",
        "selected_df_of_register_df = register_df.select(\"station\", \"dayofweek\", \"hour\", \"fullstatus\")\n",
        "\n",
        "# Виводимо вміст DataFrame\n",
        "selected_df_of_register_df.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "FOc5yKE2d_hc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a84dcca6-a49a-4019-96b9-ce551c4c1d51"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----------+-------+----------+\n",
            "|station| dayofweek|   hour|fullstatus|\n",
            "+-------+----------+-------+----------+\n",
            "|      1|2023-05-15|12:20:0|         0|\n",
            "|      1|2023-05-15|12:22:0|         0|\n",
            "|      1|2023-05-15|12:24:0|         0|\n",
            "|      1|2023-05-15|12:26:0|         0|\n",
            "|      1|2023-05-15|12:28:0|         0|\n",
            "|      1|2023-05-15|12:32:0|         0|\n",
            "|      1|2023-05-15|12:34:0|         0|\n",
            "|      1|2023-05-15|12:36:0|         0|\n",
            "|      1|2023-05-15|12:38:0|         0|\n",
            "|      1|2023-05-15|12:40:0|         0|\n",
            "|      1|2023-05-15|12:42:0|         0|\n",
            "|      1|2023-05-15|12:46:0|         0|\n",
            "|      1|2023-05-15|12:48:0|         0|\n",
            "|      1|2023-05-15|12:50:0|         0|\n",
            "|      1|2023-05-15|12:52:0|         0|\n",
            "|      1|2023-05-15|12:54:0|         0|\n",
            "|      1|2023-05-15|12:56:0|         0|\n",
            "|      1|2023-05-15|12:58:0|         0|\n",
            "|      1|2023-05-15| 13:0:0|         0|\n",
            "|      1|2023-05-15| 13:2:0|         0|\n",
            "+-------+----------+-------+----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Визначте одну групу для кожної комбінації `(station, dayofweek, hour)`"
      ],
      "metadata": {
        "id": "aS8jAVRid6qU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ваш код\n",
        "# Виконуємо реєстрацію тимчасової таблиці для подальшого використання SQL\n",
        "selected_df_of_register_df.createOrReplaceTempView(\"register_with_count\")\n",
        "\n",
        "# Формуємо SQL-запит для визначення груп для кожної комбінації (station, dayofweek, hour)\n",
        "query = \"\"\"\n",
        "    SELECT station, dayofweek, hour, COUNT(*) AS group_count\n",
        "    FROM register_with_count\n",
        "    GROUP BY station, dayofweek, hour\n",
        "\"\"\"\n",
        "\n",
        "# Виконуємо SQL-запит і отримуємо результат\n",
        "groupedby_df = spark.sql(query)\n",
        "\n",
        "# Виводимо результат\n",
        "groupedby_df.show()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "u5C4TKhieBTL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f4181cd-aece-4c43-8904-edcbffa4bbe1"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----------+-------+-----------+\n",
            "|station| dayofweek|   hour|group_count|\n",
            "+-------+----------+-------+-----------+\n",
            "|      1|2023-05-16| 6:56:0|          1|\n",
            "|      1|2023-05-16|20:54:0|          1|\n",
            "|      1|2023-05-17|12:40:0|          1|\n",
            "|      1|2023-05-17|23:34:0|          1|\n",
            "|      1|2023-05-18|20:34:0|          1|\n",
            "|      1|2023-05-19|13:44:0|          1|\n",
            "|      1|2023-05-20|13:56:0|          1|\n",
            "|      1|2023-05-20|14:34:0|          1|\n",
            "|      1|2023-05-20|19:14:0|          1|\n",
            "|      1|2023-05-21|14:44:0|          1|\n",
            "|      1|2023-05-22|14:46:0|          1|\n",
            "|      1|2023-05-22|15:30:0|          1|\n",
            "|      1|2023-05-22|16:54:0|          1|\n",
            "|      1|2023-05-22|21:22:0|          1|\n",
            "|      1|2023-05-23|10:58:0|          1|\n",
            "|      1|2023-05-24|12:20:0|          1|\n",
            "|      1|2023-05-24|19:38:0|          1|\n",
            "|      1|2023-05-25|15:54:0|          1|\n",
            "|      1|2023-05-25| 17:4:0|          1|\n",
            "|      1|2023-05-25|17:24:0|          1|\n",
            "+-------+----------+-------+-----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Обчисліть \"критичність\" для кожної групи `(station, dayofweek, hour)`, тобто для кожної пари `(station, timeslot)`.\n",
        "\n",
        "Критичність дорівнює середньому `fullStatus`."
      ],
      "metadata": {
        "id": "CyVEVcv1eDQE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ваш код\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Ініціалізуємо SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Calculate Criticality\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Реєструємо тимчасову таблицю для DataFrame\n",
        "register_df.createOrReplaceTempView(\"register\")\n",
        "\n",
        "# Виконуємо SQL запит для обчислення критичності за формулою для кожної групи (станція, день тижня, година)\n",
        "criticality_df = spark.sql(\"\"\"\n",
        "    SELECT\n",
        "        station,\n",
        "        dayofweek,\n",
        "        hour,\n",
        "        SUM(used_slots) / (SUM(used_slots) + SUM(free_slots)) AS criticality\n",
        "    FROM\n",
        "        register\n",
        "    GROUP BY\n",
        "        station,\n",
        "        dayofweek,\n",
        "        hour\n",
        "\"\"\")\n",
        "\n",
        "# Виводимо результат\n",
        "criticality_df.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "FqqjMl4weFhE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b0303fc-4ccf-418d-c1c0-76d3aaecb9f1"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----------+-------+-------------------+\n",
            "|station| dayofweek|   hour|        criticality|\n",
            "+-------+----------+-------+-------------------+\n",
            "|      1|2023-05-16| 6:56:0| 0.6666666666666666|\n",
            "|      1|2023-05-16|20:54:0|0.21428571428571427|\n",
            "|      1|2023-05-17|12:40:0|                0.1|\n",
            "|      1|2023-05-17|23:34:0| 0.3157894736842105|\n",
            "|      1|2023-05-18|20:34:0|0.26666666666666666|\n",
            "|      1|2023-05-19|13:44:0|0.47619047619047616|\n",
            "|      1|2023-05-20|13:56:0| 0.7142857142857143|\n",
            "|      1|2023-05-20|14:34:0| 0.7619047619047619|\n",
            "|      1|2023-05-20|19:14:0|0.05263157894736842|\n",
            "|      1|2023-05-21|14:44:0| 0.3157894736842105|\n",
            "|      1|2023-05-22|14:46:0| 0.8333333333333334|\n",
            "|      1|2023-05-22|15:30:0| 0.7222222222222222|\n",
            "|      1|2023-05-22|16:54:0| 0.4444444444444444|\n",
            "|      1|2023-05-22|21:22:0| 0.3888888888888889|\n",
            "|      1|2023-05-23|10:58:0|0.10526315789473684|\n",
            "|      1|2023-05-24|12:20:0|0.23809523809523808|\n",
            "|      1|2023-05-24|19:38:0|                0.2|\n",
            "|      1|2023-05-25|15:54:0|0.05263157894736842|\n",
            "|      1|2023-05-25| 17:4:0|0.15789473684210525|\n",
            "|      1|2023-05-25|17:24:0| 0.2631578947368421|\n",
            "+-------+----------+-------+-------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Виберіть лише рядки з `criticality > threshold`\n",
        "\n",
        "> `threshold` є деякою бізнес-вимогою, тому візьміть випадкове число від `0.1` до `0.5`, яке вам подобається :)"
      ],
      "metadata": {
        "id": "Kn-KHFBMeHU8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "threshold = 0.25\n",
        "# ваш код\n",
        "# Вибираємо лише рядки, де значення критичності перевищує поріг\n",
        "selected_criticality_df = criticality_df.filter(criticality_df[\"criticality\"] > threshold)\n",
        "\n",
        "# Виводимо результат\n",
        "selected_criticality_df.show()\n"
      ],
      "metadata": {
        "id": "bAwP23fVeLKM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "afcc0b61-07ff-46a3-cacd-190a8d206019"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----------+-------+-------------------+\n",
            "|station| dayofweek|   hour|        criticality|\n",
            "+-------+----------+-------+-------------------+\n",
            "|      1|2023-05-16| 6:56:0| 0.6666666666666666|\n",
            "|      1|2023-05-17|23:34:0| 0.3157894736842105|\n",
            "|      1|2023-05-18|20:34:0|0.26666666666666666|\n",
            "|      1|2023-05-19|13:44:0|0.47619047619047616|\n",
            "|      1|2023-05-20|13:56:0| 0.7142857142857143|\n",
            "|      1|2023-05-20|14:34:0| 0.7619047619047619|\n",
            "|      1|2023-05-21|14:44:0| 0.3157894736842105|\n",
            "|      1|2023-05-22|14:46:0| 0.8333333333333334|\n",
            "|      1|2023-05-22|15:30:0| 0.7222222222222222|\n",
            "|      1|2023-05-22|16:54:0| 0.4444444444444444|\n",
            "|      1|2023-05-22|21:22:0| 0.3888888888888889|\n",
            "|      1|2023-05-25|17:24:0| 0.2631578947368421|\n",
            "|      1|2023-05-25|18:52:0| 0.3333333333333333|\n",
            "|      1|2023-05-26|13:34:0| 0.2631578947368421|\n",
            "|      1|2023-05-26|21:10:0|             0.9375|\n",
            "|      1|2023-05-27|20:56:0| 0.6470588235294118|\n",
            "|      1|2023-05-28| 17:4:0|                0.4|\n",
            "|      1|2023-05-28|22:40:0| 0.2631578947368421|\n",
            "|      1|2023-05-29|  7:2:0|  0.631578947368421|\n",
            "|      1|2023-05-30| 6:48:0|                0.8|\n",
            "+-------+----------+-------+-------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Прочитайте вміст вхідного файлу `stations.csv` і збережіть його у DataFrame.\n",
        "\n",
        "Вхідний файл має заголовок.\n",
        "\n",
        "Схема даних:\n",
        "* id: integer (nullable = true)\n",
        "* longitude: double (nullable = true)\n",
        "* latitude: double (nullable = true)\n",
        "* name: string (nullable = true)"
      ],
      "metadata": {
        "id": "o6nU8H1ieNED"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ваш код\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Ініціалізуємо SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Read Stations CSV\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Читаємо вміст вхідного файлу stations.csv та зберігаємо його у DataFrame\n",
        "stations_df = spark.read.csv(\"stations.csv\", header=True, sep=\"\\t\")\n",
        "\n",
        "# Виведемо перші 5 рядків для перевірки\n",
        "stations_df.show()\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "c7ja3VZ-eQC0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d2a4639-0336-4156-c2af-94c9b913ce1d"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+---------+---------+--------------------+\n",
            "| id|longitude| latitude|                name|\n",
            "+---+---------+---------+--------------------+\n",
            "|  1| 2.180019|41.397978|Gran Via Corts Ca...|\n",
            "|  2| 2.176414|41.394381|       Plaza TetuÃ¡n|\n",
            "|  3| 2.181164| 41.39375|             Ali Bei|\n",
            "|  4|   2.1814|41.393364|               Ribes|\n",
            "|  5| 2.180214|41.391072|  Pg LluÃ­s Companys|\n",
            "|  6| 2.180508|41.391272|  Pg LluÃ­s Companys|\n",
            "|  7| 2.183183|41.388867|  Pg LluÃ­s Companys|\n",
            "|  8| 2.183453|41.389044|Passeig lluÃ­s co...|\n",
            "|  9| 2.185294|41.385006|MarquÃ¨s de l\\'Ar...|\n",
            "| 10| 2.185206|41.384875|Avinguda del Marq...|\n",
            "| 11| 2.183622|41.385394|             ComerÃ§|\n",
            "| 12| 2.193939|41.381681|            Trelawny|\n",
            "| 13| 2.195661|41.384522|pg marÃ­tim barce...|\n",
            "| 14| 2.195603|41.384417|     Passeig Maritim|\n",
            "| 15| 2.195706|41.386811|       Avda. Litoral|\n",
            "| 16| 2.195764|41.386869|    Avinguda Litoral|\n",
            "| 17| 2.170775|41.395161|              Girona|\n",
            "| 18|   2.1867|41.398258|       Av. Meridiana|\n",
            "| 19| 2.186697|41.398181|             Padilla|\n",
            "| 20| 2.174139|41.405875|           RossellÃ³|\n",
            "+---+---------+---------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Об’єднайте (`JOIN`) вибрані критичні часові інтервали з таблицею станцій, щоб отримати координати станцій"
      ],
      "metadata": {
        "id": "ULZfbQjpeRmT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ваш код\n",
        "# Реєструємо тимчасову таблицю для об'єднання\n",
        "stations_df.createOrReplaceTempView(\"stations\")\n",
        "\n",
        "# Виконуємо SQL запит для об'єднання таблиці станцій з непорожніми результатами критичності\n",
        "join_df = spark.sql(\"\"\"\n",
        "    SELECT s.*, t.*\n",
        "    FROM stations s\n",
        "    LEFT JOIN (\n",
        "        SELECT *\n",
        "        FROM selected_criticality_df\n",
        "    ) t ON s.id = t.station\n",
        "\"\"\")\n",
        "\n",
        "# Виводимо результат\n",
        "join_df.show()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "DihouXazeT5L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "383fc47d-7bfd-4489-9156-9ce7be212324"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+---------+---------+--------------------+-------+----------+-------+-------------------+\n",
            "| id|longitude| latitude|                name|station| dayofweek|   hour|        criticality|\n",
            "+---+---------+---------+--------------------+-------+----------+-------+-------------------+\n",
            "|  1| 2.180019|41.397978|Gran Via Corts Ca...|      1|2023-09-29|13:24:0| 0.2962962962962963|\n",
            "|  1| 2.180019|41.397978|Gran Via Corts Ca...|      1|2023-09-29| 6:18:0|0.37037037037037035|\n",
            "|  1| 2.180019|41.397978|Gran Via Corts Ca...|      1|2023-09-29| 5:24:0| 0.4074074074074074|\n",
            "|  1| 2.180019|41.397978|Gran Via Corts Ca...|      1|2023-09-26|12:28:0| 0.5454545454545454|\n",
            "|  1| 2.180019|41.397978|Gran Via Corts Ca...|      1|2023-09-25| 4:44:0| 0.9047619047619048|\n",
            "|  1| 2.180019|41.397978|Gran Via Corts Ca...|      1|2023-09-24|20:26:0| 0.5714285714285714|\n",
            "|  1| 2.180019|41.397978|Gran Via Corts Ca...|      1|2023-09-24|17:12:0| 0.3333333333333333|\n",
            "|  1| 2.180019|41.397978|Gran Via Corts Ca...|      1|2023-09-22| 22:6:0|               0.75|\n",
            "|  1| 2.180019|41.397978|Gran Via Corts Ca...|      1|2023-09-22| 5:32:0|               0.85|\n",
            "|  1| 2.180019|41.397978|Gran Via Corts Ca...|      1|2023-09-19|23:32:0| 0.9444444444444444|\n",
            "|  1| 2.180019|41.397978|Gran Via Corts Ca...|      1|2023-09-19|15:16:0|  0.631578947368421|\n",
            "|  1| 2.180019|41.397978|Gran Via Corts Ca...|      1|2023-09-19|  1:2:0| 0.9523809523809523|\n",
            "|  1| 2.180019|41.397978|Gran Via Corts Ca...|      1|2023-09-18| 4:28:0| 0.9523809523809523|\n",
            "|  1| 2.180019|41.397978|Gran Via Corts Ca...|      1|2023-09-16|14:44:0|0.47619047619047616|\n",
            "|  1| 2.180019|41.397978|Gran Via Corts Ca...|      1|2023-09-12|14:42:0|0.38095238095238093|\n",
            "|  1| 2.180019|41.397978|Gran Via Corts Ca...|      1|2023-09-11| 5:18:0| 0.8571428571428571|\n",
            "|  1| 2.180019|41.397978|Gran Via Corts Ca...|      1|2023-09-10| 5:12:0| 0.6666666666666666|\n",
            "|  1| 2.180019|41.397978|Gran Via Corts Ca...|      1|2023-09-09|16:18:0| 0.9523809523809523|\n",
            "|  1| 2.180019|41.397978|Gran Via Corts Ca...|      1|2023-09-08|14:42:0|               0.45|\n",
            "|  1| 2.180019|41.397978|Gran Via Corts Ca...|      1|2023-09-03| 20:6:0| 0.7894736842105263|\n",
            "+---+---------+---------+--------------------+-------+----------+-------+-------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Відсортуйте вміст DataFrame"
      ],
      "metadata": {
        "id": "XrvGyOFHeWOU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ваш код\n",
        "# Реєструємо тимчасову таблицю для обробки\n",
        "join_df.createOrReplaceTempView(\"merged_table\")\n",
        "\n",
        "# Виконуємо SQL запит для вибору потрібних стовпців та впорядкування результату\n",
        "sortedby_df = spark.sql(\"\"\"\n",
        "    SELECT station, dayofweek, hour, criticality, longitude, latitude\n",
        "    FROM merged_table\n",
        "\"\"\")\n",
        "\n",
        "# Виводимо результат\n",
        "sortedby_df.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "vg8tUkigeZqE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9bebc1d4-511d-49cb-c873-56f74c2d6228"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----------+-------+-------------------+---------+---------+\n",
            "|station| dayofweek|   hour|        criticality|longitude| latitude|\n",
            "+-------+----------+-------+-------------------+---------+---------+\n",
            "|      1|2023-09-29|13:24:0| 0.2962962962962963| 2.180019|41.397978|\n",
            "|      1|2023-09-29| 6:18:0|0.37037037037037035| 2.180019|41.397978|\n",
            "|      1|2023-09-29| 5:24:0| 0.4074074074074074| 2.180019|41.397978|\n",
            "|      1|2023-09-26|12:28:0| 0.5454545454545454| 2.180019|41.397978|\n",
            "|      1|2023-09-25| 4:44:0| 0.9047619047619048| 2.180019|41.397978|\n",
            "|      1|2023-09-24|20:26:0| 0.5714285714285714| 2.180019|41.397978|\n",
            "|      1|2023-09-24|17:12:0| 0.3333333333333333| 2.180019|41.397978|\n",
            "|      1|2023-09-22| 22:6:0|               0.75| 2.180019|41.397978|\n",
            "|      1|2023-09-22| 5:32:0|               0.85| 2.180019|41.397978|\n",
            "|      1|2023-09-19|23:32:0| 0.9444444444444444| 2.180019|41.397978|\n",
            "|      1|2023-09-19|15:16:0|  0.631578947368421| 2.180019|41.397978|\n",
            "|      1|2023-09-19|  1:2:0| 0.9523809523809523| 2.180019|41.397978|\n",
            "|      1|2023-09-18| 4:28:0| 0.9523809523809523| 2.180019|41.397978|\n",
            "|      1|2023-09-16|14:44:0|0.47619047619047616| 2.180019|41.397978|\n",
            "|      1|2023-09-12|14:42:0|0.38095238095238093| 2.180019|41.397978|\n",
            "|      1|2023-09-11| 5:18:0| 0.8571428571428571| 2.180019|41.397978|\n",
            "|      1|2023-09-10| 5:12:0| 0.6666666666666666| 2.180019|41.397978|\n",
            "|      1|2023-09-09|16:18:0| 0.9523809523809523| 2.180019|41.397978|\n",
            "|      1|2023-09-08|14:42:0|               0.45| 2.180019|41.397978|\n",
            "|      1|2023-09-03| 20:6:0| 0.7894736842105263| 2.180019|41.397978|\n",
            "+-------+----------+-------+-------------------+---------+---------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Write to file:"
      ],
      "metadata": {
        "id": "tjYc8c6iedBL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ваш код\n",
        "# Реєструємо тимчасову таблицю для обробки\n",
        "sortedby_df .createOrReplaceTempView(\"sorted_table\")\n",
        "\n",
        "# Виконуємо SQL запит для запису даних DataFrame у CSV-файл\n",
        "spark.sql(\"\"\"\n",
        "    SELECT *\n",
        "    FROM sorted_table\n",
        "\"\"\").write.csv(\"ready_data.csv\", header=True)\n"
      ],
      "metadata": {
        "id": "AGskw9jiedaj"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Зупинка Spark"
      ],
      "metadata": {
        "id": "LoU9JapXefZr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark.stop()"
      ],
      "metadata": {
        "id": "WCW63PnGeh-9"
      },
      "execution_count": 51,
      "outputs": []
    }
  ]
}