{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOP2WG6MtJttVBGo/0PPSZg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TarasMuzychuk/Practise_Task-3/blob/main/Task_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iVK-7_54Yagu",
        "outputId": "5da5015c-6ada-4469-e19c-4f3ecac98edf"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.1.tar.gz (317.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.0/317.0 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.1-py2.py3-none-any.whl size=317488491 sha256=5bc62682fc34f11b743a807e0e097b7bf92809421b799ef4b0fa8c1d035d6e2d\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/1d/60/2c256ed38dddce2fdd93be545214a63e02fbd8d74fb0b7f3a6\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Робота з Spark RDD\n",
        "\n",
        "## Найпопулярніші пари продуктів\n",
        "\n",
        "В якості датасету для завдання необхідно використати [Amazon Reviews](https://www.kaggle.com/snap/amazon-fine-food-reviews).\n",
        "\n",
        "> Для більш зручної розробки логіки в додатково в Класрумі є скорочений файл схожої структури `sample.csv`.\n",
        "\n",
        "Датасет має наступну структуру. Друга колонка - ідентифікатор продукту, третя - ідентифікатор юзера:\n",
        "\n",
        "```\n",
        "Id,ProductId,UserId,ProfileName,HelpfulnessNumerator,HelpfulnessDenominator,Score,Time,Summary,Text\n",
        "```\n",
        "\n",
        "Наприклад:\n",
        "\n",
        "| Id | ProductId | UserId | ProfileName | ...інші колонки |\n",
        "|----|-----------|--------|-------------|-----------------|\n",
        "| 1  | B1        | A2     | Patron      | ...             |\n",
        "\n",
        "### Опис завдання\n",
        "\n",
        "1. Зчитати скорочений або повний датасет як RDD. Після читання вирізати лише потрібні колонки: `UserId` та `ProductId`.\n",
        "2. Створити `RDD`, що містить пари (`tuple`) `UserId` та список всіх `ProductId` для всіх продуктів, які придбав цей юзер. В списку повинні бути лише **унікальні** продукти (`ProductId` для одного юзера не повинні повторюватись). Наприклад:\n",
        "\n",
        "```python\n",
        "(\"A1\", [\"B1\", \"B2\", \"B5\"])\n",
        "(\"A2\", [\"B1\", \"B3\", \"B5\"])\n",
        "...\n",
        "```\n",
        "\n",
        "3. Взявши списки продуктів для кожного юзера, отримати всі пари продуктів які він міг купувати разом. Для кожної такої пари створити `tuple` де першим елементом є пара, другим число `1`. Наприклад для попереднього списку:\n",
        "```python\n",
        "(\"B1,B2\", 1)\n",
        "(\"B1,B5\", 1)\n",
        "(\"B2,B5\", 1)\n",
        "(\"B1,B3\", 1)\n",
        "(\"B1,B5\", 1)\n",
        "(\"B3,B5\", 1)\n",
        "...\n",
        "```\n",
        "\n",
        "> Два продукти вважаються придбаними разом, якщо вони обидва з’являються у списку, який ви отримали на попередньому кроці.\n",
        "\n",
        "4. Підрахувати кількість всіх пар продуктів, відсортувати їх за кількістю від найбільшої до найменшої.\n",
        "5. Взяти лише перші `10` пар продуктів та їх кількість. Наприклад:\n",
        "```python\n",
        "(\"B1,B5\", 23495)\n",
        "(\"B2,B5\", 3340)\n",
        "(\"B3,B5\", 217)\n",
        "...\n",
        "```\n",
        "6. Зберегти результат в текстовий файл."
      ],
      "metadata": {
        "id": "wdfuGawGUY1H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Конфігурація\n",
        "\n",
        "- `number_cores`: Кількість ядер, виділених під Spark\n",
        "- `memory_gb`: Обʼєм оперативної памʼяті, виділеної під Spark (в Гб)\n",
        "- `is_full_dataset`: Читати повний чи скорочений датасет."
      ],
      "metadata": {
        "id": "dHYqXSMzUk4V"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "3459rmK-ULio"
      },
      "outputs": [],
      "source": [
        "number_cores = 2\n",
        "memory_gb = 4\n",
        "is_full_dataset =False"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkConf, SparkContext\n",
        "import os\n",
        "\n",
        "number_cores = 2\n",
        "memory_gb = 4\n",
        "conf = (\n",
        "    SparkConf()\n",
        "        .setAppName(\"Spark Rdd Task\")\n",
        "        .setMaster(f'local[{number_cores}]')\n",
        "        .set('spark.driver.memory', f'{memory_gb}g')\n",
        ")\n",
        "\n",
        "sc = SparkContext(conf=conf)"
      ],
      "metadata": {
        "id": "lfb-0WBbUrYm"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Рішення\n"
      ],
      "metadata": {
        "id": "_LRze0mJUypF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if is_full_dataset:\n",
        "    if not os.path.exists('Reviews.csv'):\n",
        "        sc.stop()\n",
        "        raise Exception(\"\"\"\n",
        "            Download the 'Reviews.csv' file from https://www.kaggle.com/datasets/snap/amazon-fine-food-reviews\n",
        "            and put it in 'input' folder\n",
        "        \"\"\")\n",
        "    else:\n",
        "        inputRdd = sc.textFile(\"Reviews.csv\")\n",
        "else:\n",
        "    inputRdd = sc.textFile(\"sample.csv\")"
      ],
      "metadata": {
        "id": "S_WM3RYFU0uv"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Видалення рядку заголовку\n",
        "filteredInput = inputRdd.filter(lambda line: line.startswith(\"Id,\") == False)"
      ],
      "metadata": {
        "id": "n-bH1OqgU0r9"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ваш код починається тут\n",
        "# працювати треба з RDD `filteredInput`\n",
        " # Розділяємо вхідний потік на окремі частини\n",
        "separate_parts = filteredInput.map(lambda row: row.split(','))\n",
        "\n",
        "# Створюємо пари (`Клієнт` та `Товар`)\n",
        "client_product_pairs = separate_parts.map(lambda item: (item[2], item[1]))\n",
        "\n",
        "# Групуємо за клієнтом та об'єднуємо всі товари у список унікальних продуктів\n",
        "unique_products_per_client = client_product_pairs.groupByKey().mapValues(lambda product: list(set(product)))\n",
        "unique_products_per_client.take(10)\n",
        "\n",
        "# Завантажуємо необхідні модулі\n",
        "from itertools import permutations\n",
        "from collections import defaultdict\n",
        "\n",
        "# Функція для створення пар товарів та підрахунку їх кількості\n",
        "def get_product_pairs_count(products):\n",
        "    product_pairs_count = defaultdict(int)\n",
        "    for pair in permutations(products, 2):\n",
        "        product_pairs_count[pair] += 1\n",
        "    return product_pairs_count.items()\n",
        "\n",
        "# Створюємо всі можливі пари товарів для кожного клієнта\n",
        "product_pairs_count_per_client = unique_products_per_client.flatMapValues(get_product_pairs_count)\n",
        "\n",
        "# Підсумовуємо кількість кожної пари товарів\n",
        "total_product_pairs_count = product_pairs_count_per_client.map(lambda x: (x[1], 1)).reduceByKey(lambda x, y: x + y)\n",
        "\n",
        "# Сортуємо пари товарів за кількістю від найбільшої до найменшої\n",
        "sorted_product_pairs_count = total_product_pairs_count.sortBy(lambda x: x[1], ascending=False)\n",
        "\n",
        "# Беремо лише перші 10 пар товарів та їх кількість\n",
        "product_pairs_count = sorted_product_pairs_count.take(10)\n",
        "\n",
        "# Виводимо результат\n",
        "product_pairs_count\n",
        "\n",
        "# Відкриваємо файл для запису\n",
        "with open('roduct_pairs_count_v2.txt', 'w') as file:\n",
        "    # Записуємо заголовок\n",
        "    file.write(\" 10 пар продуктів та їх кількість:\\n\")\n",
        "    # Записуємо кожну пару та її кількість\n",
        "    for pair, count in product_pairs_count:\n",
        "        file.write(f\"{pair}: {count}\\n\")\n"
      ],
      "metadata": {
        "id": "tzzgvdToU0pV"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Зупинка Spark"
      ],
      "metadata": {
        "id": "XHNeXYELVCKn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sc.stop()"
      ],
      "metadata": {
        "id": "EKwYmxp1U0gM"
      },
      "execution_count": 34,
      "outputs": []
    }
  ]
}